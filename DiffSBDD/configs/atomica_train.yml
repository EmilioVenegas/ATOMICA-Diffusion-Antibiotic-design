run_name: '128h-nocutoff-v6' # New run name
logdir: './my_logs'
wandb_params:
  mode: 'disabled'
  entity: 'my_username'
  group: 'atomica'
dataset: 'atomica_PL'
datadir: 'data/processed_atomica'
atomica_model_config: ATOMICA/pretrain/pretrain_model_config.json
atomica_model_weights: ATOMICA/pretrain/pretrain_model_weights.pt
enable_progress_bar: True
num_sanity_val_steps: 0

mode: 'pocket_conditioning'
pocket_representation: 'atomica'
batch_size: 2
lr: 5.0e-5 # Slightly higher LR, relying on scheduler
noise_factor: 1.0
n_epochs: 1000
num_workers: 8
gpus: 1
strategy: 'auto'
clip_grad: True
augment_rotation: False

augment_noise: 1.0e-4 # Reduced jitter
accumulate_grad_batches: 8
gradient_clip_val: 0.5 # More aggressive gradient clipping

auxiliary_loss: False
loss_params:
  max_weight: 0.01
  schedule: 'linear'
  clamp_lj: 1.0

egnn_params:
  #context_nf: 512
  device: 'cuda'
  edge_cutoff_ligand: null # Set explicit cutoffs
  #edge_cutoff_pocket: 5.0
  edge_cutoff_interaction: 5.0
  reflection_equivariant: True
  atomica_embed_dim: 32
  #joint_nf: 128
  hidden_nf: 128
  n_layers: 4
  attention: True
  tanh: True  
  norm_constant: 1.0 # Ensure this is a float
  inv_sublayers: 1
  sin_embedding: False
  aggregation_method: 'mean'
  normalization_factor: 1.0 # Not used for 'mean', but set to 1 for safety
  edge_embedding_dim: 32

diffusion_params:
  diffusion_steps: 500
  diffusion_noise_schedule: 'polynomial_2'
  diffusion_noise_precision: 1.0e-4 # Slightly higher precision
  diffusion_loss_type: 'l2'
  normalize_factors: [1.0, 5.0] 

eval_epochs: 50
visualize_sample_epoch: 50
visualize_chain_epoch: 50
eval_params:
  n_eval_samples: 100
  eval_batch_size: 4
  smiles_file: null
  n_visualize_samples: 5
  keep_frames: 100